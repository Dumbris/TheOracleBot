{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CISS_project",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I9ZQXimJCzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "c93f00bc-b91d-4e1e-c260-84d97dd127df"
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Total 184 (delta 0), reused 0 (delta 0), pack-reused 184\u001b[K\n",
            "Receiving objects: 100% (184/184), 4.36 MiB | 3.92 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO3K8Ep6JvUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "c3b5158e-e903-489a-a795-456e0809df4b"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.4)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpRD47hqK3od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92218f7d-2ee0-49d7-edc6-a03819cb3bd9"
      },
      "source": [
        "%cd gpt-2"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZm95VyJ1SJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "8683c96c-3ab3-46db-d082-5672b99b6ddb"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r gpt-2/requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r gpt-2/requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r gpt-2/requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r gpt-2/requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r gpt-2/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (2.8)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O4Q2d_oLHwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "162560b2-4341-4f29-d319-4c3a7cc1aac5"
      },
      "source": [
        "!python download_model.py 345M"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 979kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 38.8Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 941kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:06, 74.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 4.04Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 50.6Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 50.0Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnF91t1lJ9Nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sed -i 's/top_k=0/top_k=40/g' src/interactive_conditional_samples.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nVJ6VmYM4T7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5278213c-18e5-47c3-c08a-64857a8a5ee4"
      },
      "source": [
        "%cd src/"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJcjlZKAKmVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(seed_text=\"\", length=None) -> str:\n",
        "  import os\n",
        "  import numpy as np\n",
        "  import tensorflow as tf\n",
        "  import json\n",
        "\n",
        "  import model, sample, encoder\n",
        "\n",
        "  nsamples = 1\n",
        "  models_dir = \"../models\"\n",
        "  batch_size = 1\n",
        "  temperature = 1\n",
        "  top_k = 30\n",
        "  seed = None\n",
        "  model_name='117M'\n",
        "  model_name='345M'\n",
        "\n",
        "  models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "  if batch_size is None:\n",
        "      batch_size = 1\n",
        "  assert nsamples % batch_size == 0\n",
        "\n",
        "  enc = encoder.get_encoder(model_name, models_dir)\n",
        "  hparams = model.default_hparams()\n",
        "  with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "      hparams.override_from_dict(json.load(f))\n",
        "\n",
        "  if length is None:\n",
        "      length = hparams.n_ctx // 2\n",
        "  elif length > hparams.n_ctx:\n",
        "      raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "  with tf.Session(graph=tf.Graph()) as sess:\n",
        "      context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "      np.random.seed(seed)\n",
        "      tf.set_random_seed(seed)\n",
        "      output = sample.sample_sequence(\n",
        "          hparams=hparams, length=length,\n",
        "          context=context,\n",
        "          batch_size=batch_size,\n",
        "          temperature=temperature, top_k=top_k\n",
        "      )\n",
        "\n",
        "      saver = tf.train.Saver()\n",
        "      ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "      saver.restore(sess, ckpt)\n",
        "\n",
        "      raw_text = seed_text\n",
        "      context_tokens = enc.encode(raw_text)\n",
        "      generated = 0\n",
        "      for _ in range(nsamples // batch_size):\n",
        "          out = sess.run(output, feed_dict={\n",
        "              context: [context_tokens for _ in range(batch_size)]\n",
        "          })[:, len(context_tokens):]\n",
        "          for i in range(batch_size):\n",
        "              generated += 1\n",
        "              text = enc.decode(out[i])\n",
        "      return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TFtOdzPZHIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_future_prediction():\n",
        "  tag_seed = \"children life death money career 2021 2022 2030 by the age of 50 \"\n",
        "  input_text = tag_seed + \"In the year of 2049 I want to\"\n",
        "  time_markers = [\"Next month I will\", \"In autumn it will be\", \"By the end of winter I will\", \"Next year I have to\"]\n",
        "  time_index = 0\n",
        "  default_time_marker = \"And then \"\n",
        "\n",
        "  new_text = input_text\n",
        "  while len(new_text) - len(input_text) < 500:\n",
        "    new_text += generate_text(new_text, length=40)\n",
        "    period_position = new_text.rfind('.')\n",
        "    new_text = new_text[:period_position+1]\n",
        "    new_text += \"\\n\"\n",
        "    if time_index < len(time_markers):\n",
        "      new_text += time_markers[time_index]  \n",
        "      time_index += 1\n",
        "    else:\n",
        "      new_text += default_time_marker\n",
        "#   return new_text[len(tag_seed):]\n",
        "  return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lxPAdZqhO-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "68b86263-949f-4d5a-cdd3-43fe2beb77b5"
      },
      "source": [
        "new_text = get_future_prediction()\n",
        "print(new_text)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Next month I will be hosting another podcast, The Last Episode, on the latest news, thoughts and developments related to Bitcoin.\n",
            "In autumn it will be interesting to see how many of you choose to stick with Bitcoin, whether it is for the long or the short term.\n",
            "And then  you will have got a better idea about the nature of Bitcoin itself in the next newsletter to come out.\n",
            "And then  I plan to continue discussing the evolution of Bitcoin in the first half of 2015.\n",
            "Join and join the community and share on facebook, twitter, YouTube and other social media channels.\n",
            "And then  we will have a forum for Bitcoin discussion.\n",
            "I thank you for reading and I hope you will join in.\n",
            "I have no intention of slowing down.\n",
            "And then \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}